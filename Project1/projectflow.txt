1.a
Generate data w FF on x,y in [0,1]
OLS on data, w/wo noise N(0,1). Use polynomials up to 5th order,p.
Find confidence interval of beta from var(beta)
Q.Evaluate (OLS using different p) by MSE and R2 for different N and noise. 
1.b
Split data in test/train.
Implement k-split cross val algo alternatively bootstrap
Evaluate MSE from test data - compare with SCKITlearn
1.c
Include bias-var trade off. cExplain terms of eq., which is bias which is variance and discuss their interpretations.
Discuss the bias and variance tradeoff as function of your model complexity (figure sim. to fig 2.11)
(the degree of the polynomial) and the number of data points, and possibly also
your training and test data. Try to make a figure similar to Fig. 2.11 of Hastie, Tibshirani, and Friedman,
see the references below
1.d
Write your own code for the Ridge method either matrix inversion or SVD

1.e) Write own or use Sckikit(recommended) on Lasso. Discuss methods.
1.f) 
QUESTIONS: 
1.a) 
Find confidence interval of beta from variances VAR(beta)=1/N sum(Beta_i - bar(beta))? . 
Evaluate OLS as a function of ?p?  ?N? and ?noise?
Method: explain OLS, k-split crossval, MSE, R2 (?), Bias-Variance tradeoff

TERM
Sampling error/variation due to sampling
Inference

@misc{MHJ_LinReg,
author = {Morten Hjorth-Jensen},
title = {Lectures Notes in FYS-STK4155. Data Analysis and Machine Learning: Linear Regression and more Advanced Regression Analysis},
month = {Sep 13},
year = {2019},
url={https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/Regression.html},
publisher={Unpublished},
}

