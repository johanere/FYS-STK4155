@misc{MHJ_LinReg,
author = {Morten Hjorth-Jensen},
title = {Lectures Notes in FYS-STK4155. Data Analysis and Machine Learning: Linear Regression and more Advanced Regression Analysis},
month = {Sep 13},
year = {2019},
publisher={Unpublished},
}

@book{HastieTrevor2009TEoS,
series = {Springer Series in Statistics},
publisher = {Springer New York},
isbn = {9780387848570},
year = {2009},
title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
edition = {Second Edition},
language = {eng},
address = {New York, NY},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
keywords = {Computer Science ; Artificial Intelligence (Incl. Robotics) ; Data Mining and Knowledge Discovery ; Probability Theory and Stochastic Processes ; Statistical Theory and Methods ; Computational Biology/Bioinformatics ; Computer Appl. in Life Sciences ; Computer Science ; Mathematics},
}



@book{MurphyKevin,
series = {Adaptive computation and machine learning},
publisher = {MIT Press},
isbn = {0262018020},
year = {2012},
title = {Machine learning : a probabilistic perspective},
language = {eng},
address = {Cambridge},
author = {Murphy, Kevin P.},
keywords = {Maskinlæring; maskinlæring; statistikk; sannsynlighet; Maskinlæring},
}

@book{2017introstatlearn,
series = {Springer texts in statistics},
publisher = {Springer},
isbn = {978-1-4614-7137-0},
year = {2017},
author={James, Gareth},
title = {An Introduction to statistical learning : with applications in R},
edition = {Corrected at 8th printing 2017.},
language = {eng},
address = {New York},
keywords = {Statistikk; Maskinlæring; Matematisk statistikk; Matematisk statistikk; Statistisk dataanalyse; pensum; tma4267},
}

@techreport{FrankeRichard1979ACCo,
abstract = {<p>This report is concerned with methods for solving the scattered data interpolation problem: Given points (X sub K, Y sub k, F sub k), k = 1, ..., N, construct a smooth function, F(x,y), so that F(X sub k, Y sub k) = F sub k, K = 1, ...,N. A comparison of 29 methods for solution of this problem has been made. Each of the methods is discussed and the results of extensive testing for their properties and appropriate values of their parameters is given. Both local and global methods are considered. Comparisons of timing, storage, accuracy, visual pleasantness of the surface, and ease of implementation are made. A large number (over 200) of pages of perspective plots of surfaces are given. Suggestions for improvement of some methods are made, and methods which have poor approximation properties are identified.</p>},
year = {1979},
title = {A Critical Comparison of Some Methods for Interpolation of Scattered Data},
language = {eng},
author = {Franke, Richard},
keywords = {Statistics and Probability ; Interpolation ; Bivariate Density Functions ; Finite Element Analysis ; Comparison ; Weighting Functions ; Approximation(Mathematics) ; Data Reduction ; Computer Graphics ; Numerical Methods And Procedures ; Standard Deviation ; Smoothing ; Pe61152n},
url = {http://www.dtic.mil/docs/citations/ADA081688},
institution={Naval Postgraduate Sschool Montery CA},
}



@incollection{Ascher,
pages = {141--166},
publisher = {Society for Industrial and Applied Mathematics},
booktitle = {A First Course on Numerical Methods},
isbn = {9780898719970},
year = {2011},
title = {Linear Least Squares Problems},
language = {eng},
author = {Ascher, Uri M. and Greif, Chen},
}

@article{MehtaPankaj2019Ahli,
issn = {0370-1573},
abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
journal = {Physics Reports},
pages = {1--124},
volume = {810},
publisher = {Elsevier B.V.},
year = {2019},
title = {A high-bias, low-variance introduction to Machine Learning for physicists},
language = {eng},
author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G.R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
keywords = {Physicists – Analysis ; Artificial Neural Networks – Usage ; Artificial Neural Networks – Analysis ; Machine Learning – Analysis ; Visualization (Computer) – Analysis ; Particle Collisions – Analysis ; Machinery – Usage ; Machinery – Analysis ; Monte Carlo Methods – Analysis ; Information Visualization – Analysis;},
}

