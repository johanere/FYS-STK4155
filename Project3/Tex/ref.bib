@book{numpy, 
  title={A guide to NumPy}, 
  author={Oliphant, Travis E}, 
  volume={1}, 
  year={2006}, 
  publisher={Trelgol Publishing USA} 
}

@misc{MHJ_GD,
author = {Morten Hjorth-Jensen},
title = {Lectures Notes in FYS-STK4155. Data Analysis and Machine Learning: Optimization and Gradient Methods},
month = {Oct 4},
year = {2019},
publisher={Unpublished},
}

@article{adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
year = {2014},
title = {Adam: A Method for Stochastic Optimization},
author = {Kingma, Diederik P. and Ba, Jimmy},
keywords = {Computer Science - Learning},
journal={Conference paper at ICLR 2015}
}

@book{HastieTrevor2009TEoS,
series = {Springer Series in Statistics},
publisher = {Springer New York},
isbn = {9780387848570},
year = {2009},
title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
edition = {Second Edition},
language = {eng},
address = {New York, NY},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
keywords = {Computer Science ; Artificial Intelligence (Incl. Robotics) ; Data Mining and Knowledge Discovery ; Probability Theory and Stochastic Processes ; Statistical Theory and Methods ; Computational Biology/Bioinformatics ; Computer Appl. in Life Sciences ; Computer Science ; Mathematics},
}


@article{imblearn,
author  = {Guillaume  Lema{{\^i}}tre and Fernando Nogueira and Christos K. Aridas},
title   = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
journal = {Journal of Machine Learning Research},
year    = {2017},
volume  = {18},
number  = {17},
pages   = {1-5},
url     = {http://jmlr.org/papers/v18/16-365}
}

@article{sklearn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@book{MLMurphy,
series = {Adaptive computation and machine learning},
publisher = {MIT Press},
isbn = {0262018020},
year = {2012},
title = {Machine learning : a probabilistic perspective},
language = {eng},
address = {Cambridge},
author = {Murphy, Kevin P.},
keywords = {Maskinlæring; maskinlæring; statistikk; sannsynlighet; Maskinlæring},
}

@article{CortezPaulo,
issn = {01679236},
abstract = {We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression...},
journal = {Decision Support Systems},
pages = {547},
publisher = {Elsevier Sequoia S.A.},
year = {2009},
title = {Modeling wine preferences by data mining from physicochemical properties},
language = {eng},
address = {Amsterdam},
author = {Cortez, Paulo and Cerdeira, António and Almeida, Fernando and Matos, Telmo and Reis, José},
keywords = {Portugal ; Wines ; Preferences ; Taste ; Data Mining ; Target Markets ; Regression Analysis ; Studies ; Food Processing Industry ; Market Research ; Software & Systems ; Western Europe ; Experiment/Theoretical Treatment},
url = {http://search.proquest.com/docview/206603752/},
}

%adasyn
@inproceedings{HaiboHe2008AAss,
issn = {10987576},
abstract = {<p>This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.</p>},
pages = {1322--1328},
volume = {10},
publisher = {IEEE},
booktitle = {2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)},
isbn = {9781424418206},
year = {2008},
title = {ADASYN: Adaptive synthetic sampling approach for imbalanced learning},
language = {eng},
author = {Haibo He and Yang Bai and Garcia, E.A and Shutao Li},
keywords = {Classification Algorithms ; Decision Trees ; Algorithm Design and Analysis ; Training Data ; Machine Learning ; Accuracy ; Machine Learning Algorithms ; Computer Science},
}



@Misc{layers,
  Author = "Jeff Heaton",
  Title  = "\emph{Heaton Research: The Number of Hidden Layers}",
  Note   = "\url{https://www.heatonresearch.com/2017/06/01/hidden-layers.html}
           [2019-11-7]",
  year = 2017,
}


@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@misc{tensorflow,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}


@misc{2002PRwS,
author={S. Lee and A. Verri },
series = {Lecture Notes in Computer Science},
volume = {2388},
publisher = {Springer Berlin Heidelberg : Imprint: Springer},
isbn = {3-540-45665-1},
year = {2002},
title = {Pattern Recognition with Support Vector Machines : First International Workshop, SVM 2002, Niagara Falls, Canada, August 10, 2002. Proceedings},
edition = {1st ed. 2002.},
language = {eng},
address = {Berlin, Heidelberg},
keywords = {Pattern recognition; Optical data processing; Algorithms; Computers; Artificial intelligence; Statistics ; Pattern Recognition; Image Processing and Computer Vision; Algorithm Analysis and Problem Complexity; Computation by Abstract Devices; Artificial Intelligence; Statistics and Computing/Statistics Programs},
}

